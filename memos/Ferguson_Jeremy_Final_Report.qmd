---
title: "Final Report"
subtitle: |
  | Final Project 
  | Data Science 2 with R (STAT 301-2)
author: "Jeremy Ferguson"
date: today

format:
  html:
    toc: true
    embed-resources: true
    
execute:
  echo: false
  warning: false

from: markdown+emoji 
reference-location: margin
citation-location: margin
---

::: {.callout-tip icon=false}

## Github Repo Link

[https://github.com/stat301-2-2024-winter/final-project-2-jeremyferg.git](https://github.com/stat301-2-2024-winter/final-project-2-jeremyferg.git)

:::

```{r}
#| label: libraries and datasets
#| echo: false

library(tidyverse)
library(tidymodels)
library(here)
library(patchwork)
library(flextable)
library(pracma)
library(corrr)

tidymodels_prefer()

nba_seasons <- read_rds(here('data/nba_seasons.rds'))
nba_seasons_train <- read_rds(here('data/splits_folds/nba_seasons_train.rds'))
nba_seasons_test <- read_rds(here('data/splits_folds/nba_seasons_train.rds'))

# recipes
load(here('recipes/nba_recipe_general.rda'))
load(here('recipes/nba_recipe_outta.rda'))
load(here('recipes/nba_recipe_interact.rda'))
load(here('recipes/nba_recipe_nonlinear.rda'))

load(here('results/general_recipe/lm_fit_folds_general.rda'))
load(here('results/general_recipe/null_fit_folds_general.rda'))

# final fit
load(here('results/final_fit.rda'))


```


# Introduction

This project aims to create a predictive model that can estimate the yearly salaries of professional basketball players from the National Basketball Association (NBA). This model uses season statistics from players as its predictors. Other NBA-related factors, such as the location of the player’s team and how long the player has been in the NBA, are also used for the analysis.

This research question is a regression problem: we are trying to predict salary, a continuous outcome variable. Since we are comparing salaries across several decades, we need to make sure our measurements use one inflation rate. Therefore the target variable measures a player’s yearly salary adjusted to 2023 prices using the [Consumer Price Index for All Urban Consumers from the US Bureau of Labor Statistics.](https://data.bls.gov/pdq/SurveyOutputServlet). 

I see this research most benefiting NBA players. With this model, players would have a better understanding of what they should expect from contract offers based on their previous performance. The model also helps indicate what factors outside of the players’ control, such as what conference a team plays in, contribute towards their salaries. 

Besides its player-specific benefits, this model allows me to explore the NBA computationally. I am a great fan of the NBA (especially my hometown Chicago Bulls), so creating this model has been fun and informative of historical season statistics. Observing what statistics are highly valued when a contract is devised also gives me a deeper understanding of how organizations form championship-contending teams.

# Data Overview

Before proceeding to methodologies, it is important to check the quality of our data. Doing so will clearly lay out concerns we have about the raw data, driving how recipes, model comparison, and tuning parameters are selected and modified. Below, I use the entire dataset to explore missing values and our target variable.

## Missingness

@tbl-missing presents the number and percent of missingness for each variable. We can see that all missing observations come from the `_percent` variables. These are variables that measure the shooting percentages of players.

```{r}
#| label: tbl-missing
#| tbl-cap: Missingness in dataset

nba_seasons |> 
  naniar::miss_var_summary() |> 
  DT::datatable()

```


We can interpret this missingness as players who never took a 2-point shot, 3-point shot, or either during a season. The 3-point shot missingness does not bother me since historically players have contributed greatly to a team while never taking a 3-point shot in a season. For example, [during his MVP season in 1999-2000](https://www.basketball-reference.com/players/o/onealsh01/gamelog/2000), center Shaquille O’Neal did not attempt a single 3-point shot in the regular season or during the Los Angeles Lakers’ championship playoff run. These NAs can be replaced with 0s. We can identify players who contributed greatly to their team without shooting a 3 by filtering observations who only have missingness in the 3-point percentage category.

Those without a 2-point attempt or free-throw attempt concern me, as these players also have 0 or near 0 statistics for every other numerical predictor. @tbl-missing-fg gives a sample of the lack of data for observations in this group. 

```{r}
#| label: tbl-missing-fg
#| tbl-cap: Sample of those missing field-goal percentages in data

nba_seasons |> 
  filter(is.na(fg_percent)) |> 
  slice_head(n = 10) |> 
  DT::datatable()

```


It does not make sense to include these observations in recipes since their values are not a strong representation of how statistics change salaries. Also considering that these observations make up a small part of the dataset, the best choice is to drop them from the dataset.

## Target Variable Exploration

We start the univariate analysis of our target variable by looking at a histogram of adjusted salaries. @fig-target-histo shows this distribution. From the histogram, we can see that the majority of NBA players earn less than five million dollars in a year. The distribution does not seem to have any additional local peaks. 

```{r}
#| label: fig-target-histo

nba_seasons |> 
  ggplot(aes(adj_salary)) + 
  geom_histogram(bins = 120, color = 'white') +
  scale_x_continuous(limits = c(0, 63095401)) +
  theme_minimal() +
  labs(title = 'Histogram Distribution of Adjusted Annual NBA Salaries',
       subtitle = 'Most salaries are less than 5 million dollars',
       x = 'Adjusted Salary',
       y = '')

```


However, our distribution is right skewed. Ideally, we want our outcome variable to have a normal distribution to allow us to apply statistical properties and techniques that require a normality assumption. Reducing skewness will also make predicting values easier in our model. A common transformation for right-skewed data is a log transformation. This transformation will help reduce the skewness and deal with extreme values. The density distribution and boxp lot of adjusted salaries in @fig-target-dens-box give another perspective of the right-skewness of the data.

```{r}
#| label: fig-target-dens-box

salary_desnsity_plot <-
  nba_seasons |> 
  ggplot(aes(adj_salary)) +
  geom_density() +
  theme_minimal() +
  theme(
    axis.text.y = element_blank(),
    axis.title.y = element_blank(),
    axis.ticks.y = element_blank()
  )

salary_box_plot <-
  nba_seasons |> 
  ggplot(aes(adj_salary)) +
  geom_boxplot() +
  theme_void()

# density and boxplot of adjusted salaries
(salary_box_plot +
    labs(
      title = 'Density Distribution and Boxplot of Adjusted Salaries',
      subtitle = 'These graphs give a better understanding of where the middle-50 lies'
    ))/salary_desnsity_plot +
  plot_layout(heights = unit(c(1, 5), c('cm', 'null'))) 

```


After using the log transformation, our data looks a little left-skewed. Nevertheless, we have reduced the skewness, allowing for easier analysis. @fig-log-dens shows this left-skewness.

```{r}
#| label: fig-log-dens

(salary_box_plot + 
  labs(
    title = 'Density Distribution and Boxplot of Adjusted Salaries',
    subtitle = 'log10 transformation'
  ))/salary_desnsity_plot +
  plot_layout(heights = unit(c(1, 5), c('cm', 'null'))) &
  scale_x_log10(name = 'log10 adj_salary') 

```


We can reduce the skewness of our data even more by considering a more uncommon transformation. I found that transforming the outcome variable by the 7th root essentially removes the skewness. This transformation is visualized in @fig-nth7-dense.

```{r}
#| label: fig-nth7-dense

salary_desnsity_plot_root <-
  nba_seasons |> 
  ggplot(aes(nthroot(adj_salary, 7))) +
  geom_density() +
  theme_minimal() +
  theme(
    axis.text.y = element_blank(),
    axis.title.y = element_blank(),
    axis.ticks.y = element_blank()
  )

salary_box_plot_root <-
  nba_seasons |> 
  ggplot(aes(nthroot(adj_salary, 7))) +
  geom_boxplot() +
  theme_void()

# density and boxplot of adjusted salaries, 7th root transformation
(salary_box_plot_root +
    labs(
      title = 'Density Distribution and Boxplot of Adjusted Salaries',
      subtitle = 'Root-7 transformation'))/salary_desnsity_plot_root +
  plot_layout(heights = unit(c(1, 5), c('cm', 'null'))) 

```


While this transformation gives us a distribution close to normal, we also need to consider the interpretability of using this transformation; Explaining the findings of our model to an NBA player or agent with this transformation would be difficult. However, interpretability should not be a large concern for us since we can transform the results of our model back to conventional units. Therefore, with the desire for normality in mind, I decided to transform adjusted salaries by the 7th root.

# Methods

Again, this prediction model is a regression problem. I use RMSE as an assessment metric for this project. In this regression analysis, I am most concerned about the accuracy of my models’ estimation; I want to be able to predict NBA salaries as closely as possible, as this is the main value a player or agent is concerned with. RMSE also helps to resolve issues with outliers since the metric penalizes these observations. 

## Data Splitting
Before splitting, our data set has 13985 observations. This data set is on the smaller side, so a split should lean more towards having many training observations. Of course, we do not want our model to overfit the training data. I chose to do a .75/.25 training-testing split, as I believed this to be a good median proportion for the data. Our training set has 10440 observations, and our testing set has 3483 observations.

## Resampling Technique
I chose a v-fold cross-validation as my resampling method. The method has 10 folds repeated 5 times. I chose to use 10 folds to allow for an equilibrium between bias and variance. Again, given our data set is on the smaller side, it is not necessary to explore a larger amount of folds, as a higher value would likely create a very high variance. Repeating 5 times allows us to diminish the noisiness of our data. This technique is particularly useful for the standard errors’ accuracy within our performance metric. 

When folding, our model fits on 9396 observations and is assessed on about 1044 observations. I find these numbers to be reasonable; we have a nice number of observations for the model itself and ample observations for assessment. Through data collection, I noticed that each NBA season has about 500 players. So, we can think of this method as assessing with two seasons' worth of players.

## Model Types

My project will use the null model, linear regression model, elastic net model, random forest model, gradient boosted tree model, neural network regression model, and multivariate adaptive regression splines model. A key reason for my choices was to continuously increase flexibility while examining the change in the interpretability of my results. Also, many of the predictor variables have high correlations with each other. A few examples of these correlations are seen in @tbl-high-corr:

```{r}
#| label: tbl-high-corr
#| tbl-cap: 'Examples of high correlation'

nba_seasons_train |> 
  select(mp, fg, fga) |> 
  correlate() |> 
  knitr::kable()

```

High correlation can lead to unstable estimates and overfitting in my models, so I address this issue through my choices of methodology. An expanded analysis of this observation can be found in the EDA portion of the appendix.

### Null and Linear
The null and linear regression models are used mainly as a baseline. These models give a simple, trivial result to my data. We can assume that there are complexities in our data that need to be accounted for to produce the best performance metric. However, having baselines gives us a good indication of whether or not our other models are being hurt by complexity.

Keeping in mind the simplicity of the models, the null and linear regression models do not have hyperparameters to tune. 

### Elastic Net

The elastic net model will address some of the high correlation and multicollinearity concerns I have with the data; Since the elastic net model uses the lasso and ridge techniques to penalize multicollinearity issues, this model is great for my analysis. 

I will tune two hyperparameters for this model. The mixture hyperparameter allows us to test different proportional combinations of the lasso and ridge techniques used for penalization. The penalty hyperparameter allows us to vary the punishment a model incurs for multicollinearity issues.

### Random Forest

The random forest model is a nice median between interpretability and flexibility. The model handles overfitting concerns and outliers well. In addition, this model can handle both linear and nonlinear relationships. Exploring the training set, I saw a few predictors that may have a nonlinear relationship with the outcome variable. For example, @fig-block-nonlinear shows that the blocks predictor does not seem to have a steady, linear trend.

```{r}
#| label: fig-block-nonlinear

simple_scatter <- function(some_var, interact, two = FALSE){
  
  if(two == TRUE){
  
  nba_seasons_train |> 
    ggplot(aes({{some_var}}, adj_salary)) +
    geom_point(alpha = .1) +
    geom_smooth(aes(color = {{interact}}), se = FALSE, linewidth = 1.5) +
    theme_bw() +
    labs(
      title = rlang::englue("Scatterplot of adj_salary against {{some_var}}"),
      subtitle = rlang::englue("Grouped by {{interact}}"),
      y = '')
  
  }
  
  else{
    
    nba_seasons_train |> 
      ggplot(aes({{some_var}}, adj_salary)) +
      geom_point(alpha = .1) +
      geom_smooth(se = FALSE, linewidth = 1.5) +
      theme_bw() +
      labs(
        title = rlang::englue("Scatterplot of adj_salary against {{some_var}}"),
        y = '')
    
  }
}

simple_scatter(blk)

```

More examples are found in the EDA. If these visualizations are truly nonlinear, random forests will handle these relationships with little tuning. Concerning tuning, I vary three hyperparameters. First, I vary the number of predictors used for each decision tree. It is important to keep in mind that lower values of this hyperparameter could cause overfitting. Next, I tune the number of decision trees used in the prediction model. Allowing for a larger amount of trees should lead to more robust predictions at the cost of longer computation times. Finally, I vary the minimum number of nodes needed in each node. It is best to keep this value on the lower side, as lower values lead to more complexity required to capture the patterns of our data.

### Gradient Boosted Tree

Boosted trees allow for more complexity in our data than random forests, potentially creating more accurate performance metrics. However, the hyperparameters are quite sensitive; If the parameters are not tuned correctly, the results will not be worthwhile. This model will tune the same hyperparameters as in the random forest model. Additionally, boosted trees tunes the rate at which the model learns from previous iterations of itself. Using a lower value for this hyperparameter decreases the chance of overfitting since the weights of each tree are smaller. However, a lower value means a higher number of trees to achieve robust results.

### MARS

I am choosing to try the multivariate adaptive regression splines (MARS) model mainly for its ability to deal with nonlinearity while still having high interpretability compared to other models with a heavy nonlinearity focus. However, if our nonlinear data has many sharp changes (say one of our predictor-outcome relationships exhibits multiple peaks and troughs), the piecewise linear segments that MARS uses to capture its nonlinear relationships may not be accurate.

For this model, I am tuning two hyperparameters. The first parameter varies the number of terms that are used in the final prediction model. Increasing this value can allow us to capture more complexity. However, increasing this number too much will result in overfitting. The second hyperparameter varies the degree of the interaction term in the model. Similar to the first parameter, increasing the allowed degrees helps with capturing complexities in the model at the cost of potentially overfitting.

### Neural Network
The neural network regression model will provide more nonlinear flexibility than MARS if the intricacy of our data becomes a problem. While the model can create estimations with high accuracy, this process is very time-consuming, with multiple hyperparameters needing to be tuned. Since our data set is on the smaller side, it may be the case that neural networks actually overfit our data.

Three hyperparameters are tuned for this model. First, we vary the weight decay of the model, with higher weights preventing the model from learning overly complex patterns that are specific to the training set. Next, we adjust the number of neurons used in each layer of the model, with more neurons leading to more complexity and more potential for overfitting. Finally, we modify the number of training iterations on the entire training set that the model performs. Following the same pattern as the previous hyperparameters, larger values mean more complexity but more potential for overfitting.

## Recipes

For this project, I use four separate recipes to conclude the best-performing model. I want to examine how well various predictors and techniques do for estimating salaries. Here, I give explanations for my recipe decisions.

For all recipes, predictors with zero variance were removed. A predictor with zero variance has no distinguishing factor between observations, therefore not positively contributing to the recipe. To place all predictors on the same scale, we normalize the predictors. Also, we drop predictors that have high correlations with other predictors. Again, high correlations can lead to multicollinearity, resulting in overfitting of the model. In general, [correlations over .7 are considered high](https://www.westga.edu/academics/research/vrc/assets/docs/scatterplots_and_correlation_notes.pdf), so we remove any predictors that have correlations above this threshold.

### General

The first recipe focuses on general player statistics, such as points, assists, and rebounds. These are the predictors that average person thinks of when they imagine basketball statistics. This recipe is simplistic and minimizes the complexity of the modeling process; Only a handful of interaction terms are used and no nonlinear trends are addressed. I imagine we will get fairly reasonable predictions with this recipe, but more complexity will likely improve the results. After removing high correlations and zero-variance predictors, we end up with 21 predictors in this recipe. @tbl-gen-rec shows the predictors that end up used in this recipe:

```{r}
#| label: tbl-gen-rec
#| tbl-cap: Predictors for general recipe

nba_recipe_general |> 
  prep() |> 
  bake(new_data = NULL) |> 
  DT::datatable()

```

### Interaction

The second recipe will place a heavy focus on interaction terms. There are several predictor variables where we may expect differential effects to occur. For example, @fig-gs-ten-inter visualizes that there is a positive correlation between games started and salaries. Moreover, players who have been in the NBA for 10 or more years have higher salaries at every level along the x-axis. So, we should create a predictor that is the interaction between games played and whether a player has been in the NBA for 10 or more years.

```{r}
#| label: fig-gs-ten-inter

simple_scatter(gs, ten_years, TRUE)

```

Another key difference between this recipe and the general recipe is the use of percentage variables. For example, instead of using the variable for field goals, I use the variable for field-goal percentage. Here, I am trying to see if differing the form of the variable changes the outcome of the model. I do not expect this change to have significant effects, as we can find these types of variables through combinations of other variables. All in all, the interactions recipe has 33 predictors, as seen in @tbl-inter-rec.

```{r}
#| label: tbl-inter-rec
#| tbl-cap: Predictors for interaction recipe

nba_recipe_interact |> 
  prep() |> 
  bake(new_data = NULL) |> 
  DT::datatable()

```


### Nonlinear

The third recipe decreases the number of interaction terms used and adds nonlinearity complexity. While few, there are predictors that seem to exhibit some nonlinear trends in their distribution, as shown in @fig-block-nonlinear. While the nonlinearity is not extreme, I believe the trend is significant enough to merit a recipe that focuses specifically on this aspect of the data. 

Like the interaction recipe, the nonlinear recipe uses statistical percentages in replace of the general predictors when appropriate. The nonlinear recipe has 40 predictors, as seen in @tbl-non-rec.

```{r}
#| label: tbl-non-rec
#| tbl-cap: Predictors for nonlinear recipe

nba_recipe_nonlinear |> 
  prep() |> 
  bake(new_data = NULL) |> 
  DT::datatable()

```

### Out-of-Control

The last recipe places focus on predictors that are not completely in the player’s control, such as their team’s market size, their position, and whether or not their team makes the playoffs. Much of my reasoning for creating this recipe comes less from data exploration and more from personal theories. Although they are not directly connected to season statistics, these predictors could easily affect the accuracy of our models. It could be the case that a player who signs a contract for the New York Knicks should expect a much higher salary than if they signed for the San Antonio Spurs. This effect may simply be due to the differences in cost-of-living between various market sizes. It may also be the case that organizations in larger markets are wealthy and therefore can sign players at a market-premium.

This recipe maximizes predictors not necessarily in the control of the players and minimizes variables that are, like points, assists, etc. Neither the percentage nor non-percentage statistics are used to emphasize this minimization. The out-of-control recipe has 29 predictors, as seen in @tbl-outta-rec.

```{r}
#| label: tbl-outta-rec
#| tbl-cap: Predictors for out-of-control recipe

nba_recipe_outta |> 
  prep() |> 
  bake(new_data = NULL) |> 
  DT::datatable()

```

# Model Building & Selection

We will go through the tuning results of each recipe. It is important to note that, for comparison consistency purposes, tuning parameters were not changed relative to the recipe being used.

## Tuning Specificities 

Before continuing to the results, we should review the values used for each hyperparameter. All parameters use 5 levels. Of course, there is no need to further examine the linear and null models, as these models do not have any tuning parameters associated with them.

The elastic net model uses a mixture of lasso and ridge regressions between 0 and 1, which covers the model leaning fully towards lasso and fully towards ridge. The penalty is placed between .316 and 3.16. Here, I wanted to observe differences in the models with high and low penalties. I use the glmnet engine.

The random forest model uses 1 to 15 predictors for each decision tree, 2 to 4 nodes needed to continue the random forest, and 100 to 1000 trees. Of course, the random forest model should not use all the predictors when creating these nodes. 1 to 15 predictors meet that requirement for each of my recipes. I chose to stay conservative with minimum nodes, as we want lower values to deal with general complexity. The wide range of trees simply captures how performance changes as we increase the complexity (the number of trees) in the model. I use the ranger engine.

The boosted trees model uses 1 to 15 predictors for each tree, 1 to 10 nodes needed to continue the boosted tree, and 100 to 1000 trees. The same reasoning used in the random forest model applies here. I intended for the boosted trees model to also tune the learn rate between .1 and .95. However, I carelessly did not consider the log-transformation attached to this hyperparameter when tuning. So, the results of the boosted tree model are unfortunately mismeasured and not useful to the analysis. At the time of writing this report, there is not enough time to fully tune the boosted trees model on four different recipes. However, appropriate retuning to this model will be done in the future.

The neural networks model uses hidden units between 0 and 5, a penalty between 0 and 1, and a number of training iterations between 100 and 750. Since I observed only a small amount of nonlinearity in the data, I did not want to tune very high values in this model. As explained earlier, increasing all these values decreases the chance of overfitting while making the model more complex. It will be interesting to observe, from a nonlinearity standpoint, how complex this model will have to become before it hurts the results. I use the nnet engine.

The MARS model uses a number of predictors between 1 and 50, and degrees of freedom between 1 and 5. Predictors between 1 and 50 cover all potential sizes for each recipe used. Again, I wanted to stay conservative with nonlinearity complexity in these models since there was not an extreme abundance in my exploration. I use the earth engine.

## Individual Model Results

This section compares the performance metrics of each of the four recipes, holding one model constant. Since the null and linear models do not use hyperparameters, these models are saved for the final comparison analysis.

### Elastic Net

@tbl-nnet-gen shows the tuning parameters with the top 5 best model performances for elastic net using the general recipe. We can see that the model performs best when the mixture is 0 or close to 0. This result implies that the model strongly prefers the ridge regression concerning regularization. A smaller penalty is also preferred, leading our model closer to a linear regression model (which would have no penalty). In the future, it may be interesting to examine the RMSE when we allow for no penalty. Considering the trend of the data, we should expect a better-performing model.

```{r}
#| label: tbl-nnet-gen
#| tbl-cap: 'Best performances of elastic net (using general recipe)'

load(here("results/general_recipe/enet_general_tuned.rda"))

enet_outta_tuned |> 
  show_best(metric = 'rmse') |> 
  select(-c(.estimator, .config)) |>
  knitr::kable()

```


@tbl-nnet-inter, @tbl-nnet-non, and @tbl-nnet-outta shows the elastic net model using the interaction recipe, nonlinear recipe, and out-of-control recipes, respectfully. We can see that each of these recipes use the same combination of optimal hyperparameters as @tbl-nnet-gen. We can conclude that the hyperparameters do not have varying effects on the elastic net model when changing recipes. However, each of these recipes perform better than the general recipe; The nonlinear model gives a mean of 1.20 and the other two recipes give a mean of 1.11 (though the standard error of the interaction recipe is slightly smaller).

```{r}
#| label: tbl-nnet-inter
#| tbl-cap: 'Best performances of elastic net (using interaction recipe)'

load(here("results/interact_recipe/enet_interact_tuned.rda"))

enet_outta_tuned |> 
  show_best(metric = 'rmse') |> 
  select(-c(.estimator, .config)) |>
  knitr::kable()

```

```{r}
#| label: tbl-nnet-non
#| tbl-cap: 'Best performances of elastic net (using nonlinear recipe)'

load(here("results/nonlinear_recipe/enet_nonlinear_tuned.rda"))

enet_outta_tuned |> 
  show_best(metric = 'rmse') |> 
  select(-c(.estimator, .config)) |>
  knitr::kable()

```

```{r}
#| label: tbl-nnet-outta
#| tbl-cap: 'Best performances of elastic net (using out-of-control recipe)'

load(here("results/outta_recipe/enet_outta_tuned.rda"))

enet_outta_tuned |> 
  show_best(metric = 'rmse') |> 
  select(-c(.estimator, .config)) |>
  knitr::kable()

```

### Random Forest

@tbl-rf-gen shows the tuning parameters with the top 5 model performances for random forests using the general recipe. We can see that, for this model, the number of predictors dictates the mean of our prediction metric; How many trees and the minimum number of nodes needed in each node seem to only affect the standard error of our models. However, these standard errors are very close to each other.

```{r}
#| label: tbl-rf-gen
#| tbl-cap: 'Best performances of random forest (using general recipe)'

load(here("results/general_recipe/rf_general_tuned.rda"))

rf_outta_tuned |> 
  show_best(metric = 'rmse') |> 
  select(-c(.estimator, .config)) |>
  knitr::kable()

```

@tbl-rf-inter shows the random forests model using the interaction recipe. The number of predictors in the best model increases, which is likely a reflection of the increase of predictors in this recipe in general. The number of trees used maxes out at 1000 trees, and the minimum number of nodes needed remains at 3. The mean of this recipe is much smaller than the general recipe. For future tuning, it would be interesting to see if increasing the number of trees helps or hurts the current model performance.

```{r}
#| label: tbl-rf-inter
#| tbl-cap: 'Best performances of random forest (using interaction recipe)'

load(here("results/interact_recipe/rf_interact_tuned.rda"))

rf_outta_tuned |> 
  show_best(metric = 'rmse') |> 
  select(-c(.estimator, .config)) |>
  knitr::kable()

```

@tbl-rf-non shows the random forests model using the nonlinear recipe. In terms of hyperparameters, the only difference between this recipe and the general recipe is that the nonlinear recipe uses the maximum number of allotted predictors (15). Even with this change, the mean of the nonlinear recipe’s performance metric is equal to the general recipe’s. Moreover, the standard error is worse compared to the general recipe. These observations imply that the nonlinear model does no better than a simplistic model.

```{r}
#| label: tbl-rf-non
#| tbl-cap: 'Best performances of random forest (using nonlinear recipe)'

load(here("results/nonlinear_recipe/rf_nonlinear_tuned.rda"))

rf_outta_tuned |> 
  show_best(metric = 'rmse') |> 
  select(-c(.estimator, .config)) |>
  knitr::kable()

```

@tbl-rf-outta  shows the random forests model using the out-of-control recipe. This best performing model uses 4 predictors like the general recipe and 1000 trees like the interaction recipe. Unlike the other recipes, this model uses a minimum of 4 nodes needed to continue random forest. The mean is not as good as the interaction recipe but still a nice improvement from the other two recipes.


```{r}
#| label: tbl-rf-outta
#| tbl-cap: 'Best performances of random forest (using out-of-control recipe)'

load(here("results/outta_recipe/rf_outta_tuned.rda"))

rf_outta_tuned |> 
  show_best(metric = 'rmse') |> 
  select(-c(.estimator, .config)) |>
  knitr::kable()

```

### Boosted Tree

As mentioned in the tuning specification section, the learn rate of the boosted tree model was inaccurately tuned. This error greatly hurts the performance of all the recipes on this model.

As one example, @tbl-bt-gen shows the tuning parameters with the top 5 model performances for boosted trees using the general recipe. We can see that the learn rate is exceptionally high, the exact opposite of what we want if we are trying to minimize overfitting. Since the learn rate is so high, the boosted trees model needs less trees and utilizes less predictors compared to the random forest model. Again, these findings are not an accurate representation of the best performance with my intended hyperparameters. While not ideal, I believe that we have enough model-recipe combinations to drop boosted trees and still maintain robustness in our prediction model.

```{r}
#| label: tbl-bt-gen
#| tbl-cap: 'Best performances of boosted trees (using general recipe)'

load(here("results/general_recipe/bt_general_tuned.rda"))

bt_outta_tuned |> 
  show_best(metric = 'rmse') |> 
  select(-c(.estimator, .config)) |>
  knitr::kable()

```

### Neural Network

@tbl-nn-gen shows the tuning parameters with the top 5 model performances for neural networks using the general recipe. This model prefers the maximum hyperparameters I specified in the tuning process: 5 hidden units, a penalty of 1, and 750 training iterations on the entire training set. Hidden units and penalties seem to strongly dictate the mean of RMSE while the training iterations vary the standard error. Given the model prefers its maximized values, we should consider raising each of these values in future tuning. I expect the performance metric to be better in this setting. 

```{r}
#| label: tbl-nn-gen
#| tbl-cap: 'Best performances of neural network (using general recipe)'

load(here("results/general_recipe/ann_general_tuned.rda"))

ann_outta_tuned |> 
  show_best(metric = 'rmse') |> 
  select(-c(.estimator, .config)) |>
  knitr::kable()

```

@tbl-nn-inter shows the neural network model using the interaction recipe. For the best performing model, hidden units and penalty hyperparameters remain at 5 and 1, respectively. The number of training iterations decreases to 587. Like previous models, the mean of RMSE is lower when using the interaction recipe compared to the general recipe.

```{r}
#| label: tbl-nn-inter
#| tbl-cap: 'Best performances of neural network (using interaction recipe)'

load(here("results/interact_recipe/ann_interact_tuned.rda"))

ann_outta_tuned |> 
  show_best(metric = 'rmse') |> 
  select(-c(.estimator, .config)) |>
  knitr::kable()

```

@tbl-nn-non shows the neural network model using the nonlinear recipe. Following the same trend as found in the random forest model, the best performing model for the neural network model using the nonlinear recipe is the same as the general recipe. Means are the same, and the standard error for the general recipe is slightly lower. It is clear that the nonlinear recipe is not a good candidate for the best performing model.

```{r}
#| label: tbl-nn-non
#| tbl-cap: 'Best performances of neural network (using nonlinear recipe)'

load(here("results/nonlinear_recipe/ann_nonlinear_tuned.rda"))

ann_outta_tuned |> 
  show_best(metric = 'rmse') |> 
  select(-c(.estimator, .config)) |>
  knitr::kable()

```

@tbl-nn-outta shows the neural network model using the out-of-control recipe. Like the general and nonlinear model, the hyperparamters for the best performing model all max out my tuning parameters. Similar to previous models, the mean of this recipe is the 2nd-best performing (interactions recipe performs better).

```{r}
#| label: tbl-nn-outta
#| tbl-cap: 'Best performances of neural network (using out-of-control recipe)'

load(here("results/outta_recipe/ann_outta_tuned.rda"))

rf_outta_tuned |> 
  show_best(metric = 'rmse') |> 
  select(-c(.estimator, .config)) |>
  knitr::kable()

```

### MARS

@tbl-mar-gen shows the tuning parameters with the top 5 model performances for MARS using the general recipe. This model performs the best with 37 terms and 2 degrees of freedom. These observations imply that it prefers higher numbers of predictors with lower degrees of freedom. Therefore, nonlinear complexity is unnecessary with this recipe, which we should expect to find.

```{r}
#| label: tbl-mar-gen
#| tbl-cap: 'Best performances of MARS (using general recipe)'

load(here("results/general_recipe/mars_general_tuned.rda"))

mars_general_tuned |> 
  show_best(metric = 'rmse') |> 
  select(-c(.estimator, .config)) |>
  knitr::kable()

```

@tbl-mar-inter shows the MARS model using the interaction recipe. We can see that the best model here maximizes the allotted values of our hyperparameters. The mean and standard errors are better in this model as well. It seems that the introduction of these new interaction terms creates the need for higher degrees off freedom. Future tuning of the hyperparameters would increase these values to see if performance increases further.

```{r}
#| label: tbl-mar-inter
#| tbl-cap: 'Best performances of MARS (using interaction recipe)'

load(here("results/interact_recipe/mars_interact_tuned.rda"))

mars_interact_tuned |> 
  show_best(metric = 'rmse') |> 
  select(-c(.estimator, .config)) |>
  knitr::kable()

```

@tbl-mar-non shows the MARS model using the nonlinear recipe. We observe that the best hyperparameters mean for this recipe and the general recipe are the same. Furthermore, the standard errors are nearly identical. This observation implies that the additional complexity of the nonlinear recipe is unnecessary when using the MARS model. 

```{r}
#| label: tbl-mar-non
#| tbl-cap: 'Best performances of MARS (using nonlinear recipe)'

load(here("results/nonlinear_recipe/mars_nonlinear_tuned.rda"))

mars_nonlinear_tuned |> 
  show_best(metric = 'rmse') |> 
  select(-c(.estimator, .config)) |>
  knitr::kable()

```

@tbl-mars-outta shows the MARS model using the out-of-control recipe. The best model follows the same pattern in terms of 37 terms hyperparameter. Degrees of freedom are set at 4, likely do to the increase in interaction terms in this recipe compared to the general and nonlinear recipes. The mean of this recipe is also second to the interaction recipe.

```{r}
#| label: tbl-mars-outta
#| tbl-cap: 'Best performances of MARS (using out-of-control recipe)'

load(here("results/outta_recipe/mars_outta_tuned.rda"))

mars_outta_tuned |> 
  show_best(metric = 'rmse') |> 
  select(-c(.estimator, .config)) |>
  knitr::kable()

```

## Grouping Together 

The analysis above clearly shows that the nonlinear recipe will not be the best recipe for our model; In every model, the more simplistic general recipe works as well or better than the nonlinear model. So we can leave those results out of this analysis. @tbl-compare-table show the best RMSE for each model in the general, interaction, and out-of-control recipe. Given the methodology of our project, we need to select the best performing model-recipe combination. When dealing with RMSE, the smaller the value the better.

```{r}
#| label: tbl-compare-table
#| tbl-cap: 'Comparison of best performing models across recipes'

# due to naming errors when fitting, I have to load each tuning group each time I want
## to access another one

## general recipe tunes
load(here("results/general_recipe/ann_general_tuned.rda"))
load(here("results/general_recipe/bt_general_tuned.rda"))  
load(here("results/general_recipe/enet_general_tuned.rda"))    
load(here("results/general_recipe/lm_fit_folds_general.rda"))  
load(here("results/general_recipe/mars_general_tuned.rda"))
load(here("results/general_recipe/null_fit_folds_general.rda"))
load(here("results/general_recipe/rf_general_tuned.rda"))

gen_table <-
rbind(
  
  ann_outta_tuned |> 
    show_best(metric = 'rmse') |> 
    slice_head() |> 
    select(c(.metric, mean, n, std_err)) |>
    mutate(model = 'ANN', .before = .metric),
  
  bt_outta_tuned |> 
    show_best(metric = 'rmse') |> 
    slice_head() |> 
    select(c(.metric, mean, n, std_err)) |>
    mutate(model = 'Boosted Trees', .before = .metric),
  
  enet_outta_tuned |> 
    show_best(metric = 'rmse') |> 
    slice_head() |> 
    select(c(.metric, mean, n, std_err)) |>
    mutate(model = 'Elastic Net', .before = .metric),
  
  lm_fit_folds |> 
    show_best(metric = 'rmse') |> 
    slice_head() |> 
    select(c(.metric, mean, n, std_err)) |>
    mutate(model = 'OLS', .before = .metric),
  
  mars_general_tuned |> 
    show_best(metric = 'rmse') |> 
    slice_head() |> 
    select(c(.metric, mean, n, std_err)) |> 
    mutate(model = 'MARS', .before = .metric),
  
  null_fit_folds |> 
    show_best(metric = 'rmse') |> 
    slice_head() |> 
    select(c(.metric, mean, n, std_err)) |> 
    mutate(model = 'Null', .before = .metric),
  
  rf_outta_tuned |> 
    show_best(metric = 'rmse') |> 
    slice_head() |> 
    select(c(.metric, mean, n, std_err)) |> 
    mutate(model = 'Random Forest', .before = .metric)
  
) |> 
  mutate(recipe = 'General', .before = model)

## ioteraction recipe tunes
load(here("results/interact_recipe/ann_interact_tuned.rda"))
load(here("results/interact_recipe/bt_interact_tuned.rda"))  
load(here("results/interact_recipe/enet_interact_tuned.rda"))    
load(here("results/interact_recipe/lm_fit_folds_interact.rda"))  
load(here("results/interact_recipe/mars_interact_tuned.rda"))
load(here("results/interact_recipe/null_fit_folds_interact.rda"))
load(here("results/interact_recipe/rf_interact_tuned.rda"))     

interact_table <-
rbind(
  
  ann_outta_tuned |> 
    show_best(metric = 'rmse') |> 
    slice_head() |> 
    select(c(.metric, mean, n, std_err)) |>
    mutate(model = 'ANN', .before = .metric),
  
  bt_outta_tuned |> 
    show_best(metric = 'rmse') |> 
    slice_head() |> 
    select(c(.metric, mean, n, std_err)) |>
    mutate(model = 'Boosted Trees', .before = .metric),
  
  enet_outta_tuned |> 
    show_best(metric = 'rmse') |> 
    slice_head() |> 
    select(c(.metric, mean, n, std_err)) |>
    mutate(model = 'Elastic Net', .before = .metric),
  
  lm_fit_folds |> 
    show_best(metric = 'rmse') |> 
    slice_head() |> 
    select(c(.metric, mean, n, std_err)) |>
    mutate(model = 'OLS', .before = .metric),
  
  mars_interact_tuned |> 
    show_best(metric = 'rmse') |> 
    slice_head() |> 
    select(c(.metric, mean, n, std_err)) |> 
    mutate(model = 'MARS', .before = .metric),
  
  null_fit_folds |> 
    show_best(metric = 'rmse') |> 
    slice_head() |> 
    select(c(.metric, mean, n, std_err)) |> 
    mutate(model = 'Null', .before = .metric),
  
  rf_outta_tuned |> 
    show_best(metric = 'rmse') |> 
    slice_head() |> 
    select(c(.metric, mean, n, std_err)) |> 
    mutate(model = 'Random Forest', .before = .metric)
  
) |> 
  mutate(recipe = 'Interaction', .before = model)

## outta recipe tunes
load(here("results/outta_recipe/ann_outta_tuned.rda"))
load(here("results/outta_recipe/bt_outta_tuned.rda"))  
load(here("results/outta_recipe/enet_outta_tuned.rda"))    
load(here("results/outta_recipe/lm_fit_folds_outta.rda"))  
load(here("results/outta_recipe/mars_outta_tuned.rda"))
load(here("results/outta_recipe/null_fit_folds_outta.rda"))
load(here("results/outta_recipe/rf_outta_tuned.rda"))  

outta_table <-
rbind(
  
  ann_outta_tuned |> 
    show_best(metric = 'rmse') |> 
    slice_head() |> 
    select(c(.metric, mean, n, std_err)) |>
    mutate(model = 'ANN', .before = .metric),
  
  bt_outta_tuned |> 
    show_best(metric = 'rmse') |> 
    slice_head() |> 
    select(c(.metric, mean, n, std_err)) |>
    mutate(model = 'Boosted Trees', .before = .metric),
  
  enet_outta_tuned |> 
    show_best(metric = 'rmse') |> 
    slice_head() |> 
    select(c(.metric, mean, n, std_err)) |>
    mutate(model = 'Elastic Net', .before = .metric),
  
  lm_fit_folds |> 
    show_best(metric = 'rmse') |> 
    slice_head() |> 
    select(c(.metric, mean, n, std_err)) |>
    mutate(model = 'OLS', .before = .metric),
  
  mars_outta_tuned |> 
    show_best(metric = 'rmse') |> 
    slice_head() |> 
    select(c(.metric, mean, n, std_err)) |> 
    mutate(model = 'MARS', .before = .metric),
  
  null_fit_folds |> 
    show_best(metric = 'rmse') |> 
    slice_head() |> 
    select(c(.metric, mean, n, std_err)) |> 
    mutate(model = 'Null', .before = .metric),
  
  rf_outta_tuned |> 
    show_best(metric = 'rmse') |> 
    slice_head() |> 
    select(c(.metric, mean, n, std_err)) |> 
    mutate(model = 'Random Forest', .before = .metric)
  
) |> 
  mutate(recipe = 'Out-of_control', .before = model)

compare_table <-
  rbind(gen_table, interact_table, outta_table)

compare_table |> 
  arrange(model) |> 
  DT::datatable()

```

Though it may already be apparent from the table that the interaction recipe tends to perform the best, we can visualize the mean and standard errors of each group. @fig-com-tog visualizes the performance metric of each of our models using the three targeted recipes.   

```{r}
#| label: fig-com-tog

compare_table |> 
  ggplot(aes(x = model, y = mean)) +
  geom_errorbar(aes(ymin = mean - 1.96*std_err,
                    ymax = mean + 1.96*std_err,
                    color = recipe),
                width = 0.2) +
  geom_point(aes(color = recipe)) +
  labs(
    title = 'Performance Metrics Distributions',
    subtitle = 'The interaction recipe models tend to perform better',
    y = 'rmse'
  )

```


The figure helps us see that many of the models from the interaction recipe produced smaller RMSE values in comparison to other results. Looking at the performance metrics of the interaction recipe, we can see that neural network, MARS, and random forests all have the same minimizing mean with slightly different standard errors. It is important to note that all models using this recipe maxed out on the range of hyperparameter values I used, implying that each of these metrics could have been smaller had we expanded the tuning ranges. To me, choosing the winning model comes down to intrepreatablity, simplicity and time cost. The random forest model is easier to interpret compared to the other two models, as this model mirrors decision-making. Random forest is also more simplistic and cost effective than the nonlinear natural of neural networks and MARS. Therefore, the winning model in my project is the random forest model using the interaction recipe. This finding does not surprise me, as most of the complexity in this model was from interactions between predictors. Knowing this model can capture some nonlinearity, random forest easily interpreted the rather minimal amount of nonlinear terms in the data. 

# Final Model Analysis

As mentioned in the target variable exploration, I transitioned salaries by the 7th root for normality purposes. While we should analysis the transformed version of the final fitted model, we should also analyze the target variable on the original fitted scale. This analysis will provide us with further interpretation of the results.

## Transformation results

The metrics for the root-mean squared error, the mean absolute error, and R-squared are shown in @tbl-per-met-nth7. The interpretation of RMSE and MAE is that the average difference between observed salaries and the predicted salaries of the random forest model is about .449 and .335, respectively. Of course the nature of our transform makes it difficult to interpret these values in dollar terms. It is not surprising that MAE is greater than RMSE, does not create as large or a penalty for outliers like RMSE does. So, our results may imply the presence of salary outliers. The interpretation of RSQ is that about 93% of the variability of the observed data can be explained by our random forest model, which is a great value to see. 

```{r}
#| label: tbl-per-met-nth7
#| tbl-cap: 'Performance metrics of final model (7th-root transformed)'

# metrics set
seasons_metrics <- metric_set(rmse, mae, rsq)

# predicting  
seasons_test_res <- predict(final_fit, nba_seasons_test)

# binding prediction with actual value
seasons_test_res <- 
  bind_cols(seasons_test_res, nba_seasons_test) |> 
  select(adj_salary, .pred)

# metric set tibble
seasons_metrics(seasons_test_res, truth = adj_salary, estimate = .pred) |> 
  knitr::kable()

```

@fig-per-scat-nth7 gives us a visualization of predicted salaries against actual salaries. While the actual dollar amounts are still unclear, this scatterplot gives a better understanding of accuracy of our model in terms of variability. Estimate errors also seem to vary between the predictions overestimating and underestimating the actual values. Performing an analysis on the original scale should clear up the interpretations. 

```{r}
#| label: fig-per-scat-nth7

ggplot(bind_cols(seasons_test_res |> select(.pred), nba_seasons_test), aes(x = adj_salary, y = .pred)) + 
  # Create a diagonal line:
  geom_abline(lty = 2) + 
  geom_point(alpha = 0.1) + 
  theme_bw() +
  labs(
    title = 'Observed Versus Predicted Values for Random Forest Model',
    subtitle = '7th Root Transformed',
    y = "Predicted Salaries", 
    x = "Salaries") +
  # Scale and size the x- and y-axis uniformly:
  coord_obs_pred() 

```

## Original Results

Again, we show the metrics for the root-mean squared error, the mean absolute error, and R-squared are shown in @tbl-per-met-og, this time examining salaries on the original scale. The interpretation of RMSE and MAE is that the average difference between observed salaries and the predicted salaries of the random forest model is about 2,543,555 and 1,363,147  dollars, respectively. Generally speaking, these seem like rather large errors. It is important to consider the fact that NBA players make millions of dollars each season, so seeing errors in th millions is not unimaginable. However, I would have liked to see these errors smaller — closer to 1 million dollars. Our results still imply the presence of salary outliers. The interpretation of RSQ is that about 93.4% of the variability of the observed data can be explained by our random forest model. 

```{r}
#| label: tbl-per-met-og
#| tbl-cap: 'Performance metrics of final model (original sclae)'

# transforming back to original
seasons_test_res_og <- 
  seasons_test_res |> 
  mutate(adj_salary = adj_salary**7,
         .pred = .pred**7)

nba_seasons_test_og <- 
  nba_seasons_test |> 
  mutate(adj_salary = adj_salary**7)

seasons_metrics(seasons_test_res_og, truth = adj_salary, estimate = .pred) |> 
  knitr::kable()

```

@fig-per-scat-og gives us a visualization of predicted salaries against actual salaries using the original scale. What may actually be reassuring is that estimate errors tend to underestimate the actual values. This pattern occurs especially as salaries increase on the x-axis. If we know that predicted salaries are underestimates of what players typically receive, an NBA player can simply take this prediction and raise its value a 1 or 2 million dollars. If the model predicts smaller salaries, a player should feel more assured that the model’s prediction is not riddled with errors.

```{r}
#| label: fig-per-scat-og

## original
ggplot(bind_cols(seasons_test_res_og |> select(.pred), nba_seasons_test_og), aes(x = adj_salary, y = .pred)) + 
  # Create a diagonal line:
  geom_abline(lty = 2) + 
  geom_point(alpha = 0.1) + 
  theme_bw() +
  labs(
    title = 'Observed Versus Predicted Values for Random Forest Model',
    subtitle = 'Original Values',
    y = "Predicted Salaries", 
    x = "Salaries") +
  # Scale and size the x- and y-axis uniformly:
  coord_obs_pred() 

```

## Was it Worth it?

Based on our final model analysis, it seems like creative a predicative model was indeed worth the project. While the estimate errors were less than ideal for me, high variability shows that the model fits our data exceptionally well. The model gives us at the least a solid prediction of salaries that a NBA player or agent can adjust when bargaining for contracts. The random forest model did particularly well in predicting salaries because its ability to deal with small nonlinear variations while retaining a bit of simplicity in the process. We did not need the nonlinearity complexity that MARS and neural network brings but we did need more linear complexity that a null/linear model cannot capture, such as overfitting issues.

# Conclusion

This project successfully created a predictive model that can estimate NBA seasonal salaries based on season statistics of a player. Much of the project’s work was finding a good level of complexity for a model and a model’s recipe. I found it interesting that even though there are many intricate basketball-related statistics, fairly simple metrics and interactions between them can explain a significant portion of expected salaries. With this model, I imagine the bargaining process of NBA contracts to be easier for a NBA players and agents.   

Understanding hyperparameters and how to appropriately select them was certainly the greatest challenge for me. While I believe I did a solid job with my results, there is still room to re-tune a lot of my models. Retuning the models could lead to lower RMSE values, alleviating my dissatisfaction with my value of RMSE in my final fit model. Due to time constraints, those adjustments cannot happen in this report. However, the work in this project is continuous, and I can continue to tune my hyperparamters in the future. 


